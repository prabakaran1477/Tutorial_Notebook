{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 27 07:46:48 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   69C    P0    30W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: textsearch in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.5/dist-packages (from textsearch)\n",
      "Requirement already satisfied: Unidecode in /usr/local/lib/python3.5/dist-packages (from textsearch)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from nltk)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.5/dist-packages (from beautifulsoup4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!sudo pip3 install contractions\n",
    "!sudo pip3 install textsearch\n",
    "!sudo pip3 install tqdm\n",
    "!sudo pip3 install nltk\n",
    "!sudo pip3 install beautifulsoup4\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      "review       50000 non-null object\n",
      "sentiment    50000 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(r'https://github.com/dipanjanS/nlp_workshop_dhs18/raw/master/Unit%2011%20-%20Sentiment%20Analysis%20-%20Unsupervised%20Learning/movie_reviews.csv.bz2', compression='bz2')\n",
    "dataset['sentiment'] = [1 if record == 'positive' else 0 for record in dataset['sentiment']]\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000,), (5000,), (40000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = dataset['review'].values\n",
    "sentiments = dataset['sentiment'].values\n",
    "\n",
    "train_reviews = reviews[:5000]\n",
    "val_reviews = reviews [5000:10000]\n",
    "test_reviews = reviews[10000:]\n",
    "\n",
    "\n",
    "\n",
    "train_sentiments = sentiments[:5000]\n",
    "val_sentiments = sentiments [5000:10000]\n",
    "test_sentiments = sentiments[10000:]\n",
    "\n",
    "train_reviews.shape, val_reviews.shape, test_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def pre_process_corpus(docs):\n",
    "    norm_docs = []\n",
    "    for doc in tqdm.tqdm(docs):\n",
    "        doc = strip_html_tags(doc)\n",
    "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "        doc = doc.lower()\n",
    "        doc = remove_accented_chars(doc)\n",
    "        doc = contractions.fix(doc)\n",
    "        # lower case and remove special characters\\whitespaces\n",
    "        doc = re.sub(r'[^a-zA-Z0-9\\s]', ' ', doc, re.I|re.A)\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        doc = doc.strip()  \n",
    "        norm_docs.append(doc)\n",
    "    return norm_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:02<00:00, 2094.18it/s]\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 2107.12it/s]\n",
      "100%|██████████| 40000/40000 [00:19<00:00, 2081.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.9 s, sys: 232 ms, total: 24.1 s\n",
      "Wall time: 24 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "norm_train_texts = pre_process_corpus(train_reviews)\n",
    "norm_val_texts = pre_process_corpus(val_reviews)\n",
    "norm_test_texts = pre_process_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.keras.preprocessing.text.Tokenizer(oov_token='<UNK>')\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(norm_train_texts)\n",
    "t.word_index['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('unscarred', 39231) ('<PAD>', 0) 1\n"
     ]
    }
   ],
   "source": [
    "print(max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), \n",
    "      min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), \n",
    "      t.word_index['<UNK>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = t.texts_to_sequences(norm_train_texts)\n",
    "val_sequences = t.texts_to_sequences(norm_val_texts)\n",
    "test_sequences = t.texts_to_sequences(norm_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size=39232\n",
      "Number of Documents=5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size={}\".format(len(t.word_index)))\n",
    "print(\"Number of Documents={}\".format(t.document_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist([len(doc.split()) for doc in norm_train_texts], bins=30);\n",
    "plt.xlim([0, 2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 500), (40000, 500))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "# pad dataset to a maximum review length in words\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_val = tf.keras.preprocessing.sequence.pad_sequences(val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(t.word_index)\n",
    "EMBED_SIZE = 300\n",
    "EPOCHS=100\n",
    "BATCH_SIZE=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our attention layer for later\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        \"\"\"\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = tf.keras.initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = tf.keras.regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = tf.keras.regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = tf.keras.constraints.get(W_constraint)\n",
    "        self.b_constraint = tf.keras.constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "        \n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = tf.keras.backend.reshape(tf.keras.backend.dot(tf.keras.backend.reshape(x, \n",
    "                                                                                     (-1, features_dim)),\n",
    "                                                            tf.keras.backend.reshape(self.W, \n",
    "                                                                                     (features_dim, 1))),\n",
    "                                        (-1, step_dim))\n",
    "        \n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = tf.keras.backend.tanh(eij)\n",
    "\n",
    "        a = tf.keras.backend.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= tf.keras.backend.cast(mask, tf.keras.backend.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= tf.keras.backend.cast(tf.keras.backend.sum(a, axis=1, keepdims=True) + tf.keras.backend.epsilon(), \n",
    "                                                                                    tf.keras.backend.floatx())\n",
    "        a = tf.keras.backend.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        \n",
    "        return tf.keras.backend.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'step_dim': self.step_dim}\n",
    "        base_config = super(AttentionLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = t.word_index\n",
    "FASTTEXT_INIT_EMBEDDINGS_FILE = './wiki-news-300d-1M-subword.vec'\n",
    "\n",
    "\n",
    "def load_pretrained_embeddings(word_to_index, max_features, embedding_size, embedding_file_path):    \n",
    "    \n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*row.split(\" \")) \n",
    "                                for row in open(embedding_file_path, encoding=\"utf8\", errors='ignore') \n",
    "                                    if len(row)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    nb_words = min(max_features, len(word_to_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_size))\n",
    "    \n",
    "    for word, idx in word_to_index.items():\n",
    "        if idx >= max_features: \n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39232, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_embeddings = load_pretrained_embeddings(word_to_index=word2idx, \n",
    "                                           max_features=VOCAB_SIZE, \n",
    "                                           embedding_size=EMBED_SIZE, \n",
    "                                           embedding_file_path=FASTTEXT_INIT_EMBEDDINGS_FILE)\n",
    "ft_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 500, 300)          11769600  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 500, 512)          1140736   \n",
      "_________________________________________________________________\n",
      "attention_layer_1 (Attention (None, 512)               1012      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 13,108,725\n",
      "Trainable params: 13,108,725\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH, ))\n",
    "embed = tf.keras.layers.Embedding(VOCAB_SIZE, \n",
    "                                  EMBED_SIZE, \n",
    "                                  weights=[ft_embeddings],\n",
    "                                  trainable=True)(inp)\n",
    "bigru = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=256, return_sequences=True))(embed)\n",
    "attn = AttentionLayer(MAX_SEQUENCE_LENGTH)(bigru)\n",
    "dense1 = tf.keras.layers.Dense(256, activation='relu')(attn)\n",
    "drop1 = tf.keras.layers.Dropout(0.25)(dense1)\n",
    "dense2 = tf.keras.layers.Dense(256, activation='relu')(drop1)\n",
    "drop2 = tf.keras.layers.Dropout(0.25)(dense2)\n",
    "outp = tf.keras.layers.Dense(1, activation='sigmoid')(drop2)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inp, outputs=outp)\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "5000/5000 [==============================] - 22s 4ms/sample - loss: 0.6897 - accuracy: 0.5374 - val_loss: 0.6737 - val_accuracy: 0.5184\n",
      "Epoch 2/100\n",
      "5000/5000 [==============================] - 18s 4ms/sample - loss: 0.5334 - accuracy: 0.7374 - val_loss: 0.3528 - val_accuracy: 0.8486\n",
      "Epoch 3/100\n",
      "5000/5000 [==============================] - 18s 4ms/sample - loss: 0.1841 - accuracy: 0.9344 - val_loss: 0.3331 - val_accuracy: 0.8708\n",
      "Epoch 4/100\n",
      "5000/5000 [==============================] - 18s 4ms/sample - loss: 0.0556 - accuracy: 0.9814 - val_loss: 0.4700 - val_accuracy: 0.8590\n",
      "Epoch 5/100\n",
      "5000/5000 [==============================] - 18s 4ms/sample - loss: 0.0170 - accuracy: 0.9952 - val_loss: 0.5902 - val_accuracy: 0.8546\n",
      "Epoch 6/100\n",
      "4992/5000 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9986Restoring model weights from the end of the best epoch.\n",
      "5000/5000 [==============================] - 18s 4ms/sample - loss: 0.0056 - accuracy: 0.9986 - val_loss: 0.6845 - val_accuracy: 0.8668\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f47bc6fe5c0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                      patience=3,\n",
    "                                      restore_best_weights=True,\n",
    "                                      verbose=1)\n",
    "\n",
    "model.fit(X_train, train_sentiments, \n",
    "          validation_data=(X_val, val_sentiments),\n",
    "          epochs=EPOCHS, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          shuffle=True,\n",
    "          callbacks=[es],\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.43%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.91      0.87     20028\n",
      "          1       0.90      0.82      0.86     19972\n",
      "\n",
      "avg / total       0.87      0.86      0.86     40000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>18189</td>\n",
       "      <td>1839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3588</td>\n",
       "      <td>16384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1\n",
       "0  18189   1839\n",
       "1   3588  16384"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "predictions = [1 if pr > 0.5 else 0 \n",
    "                   for pr in model.predict(X_test, batch_size=512, verbose=0).ravel()]\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(test_sentiments, predictions)*100))\n",
    "print(classification_report(test_sentiments, predictions))\n",
    "pd.DataFrame(confusion_matrix(test_sentiments, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
